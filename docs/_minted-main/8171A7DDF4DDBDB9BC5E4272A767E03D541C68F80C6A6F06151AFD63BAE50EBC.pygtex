\begin{Verbatim}[commandchars=\\\{\}]
    \PYG{c+c1}{\PYGZsh{} carry out stochastic gradient descent over a number of epochs to train a model}
    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{examples}\PYG{p}{,} \PYG{n}{mini\PYGZus{}batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{num\PYGZus{}epochs}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{p}{):}
        \PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{num\PYGZus{}epochs}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{):}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}training epoch }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{epoch}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{num\PYGZus{}epochs}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{...\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} log message}
            \PYG{c+c1}{\PYGZsh{} randomly split into mini batches}
            \PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{examples}\PYG{p}{)}
            \PYG{n}{mini\PYGZus{}batches} \PYG{o}{=} \PYG{p}{[}\PYG{n}{examples}\PYG{p}{[(}\PYG{n}{batch\PYGZus{}number} \PYG{o}{*} \PYG{n}{mini\PYGZus{}batch\PYGZus{}size}\PYG{p}{):((}\PYG{n}{batch\PYGZus{}number}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{n}{mini\PYGZus{}batch\PYGZus{}size}\PYG{p}{)]} \PYG{k}{for} \PYG{n}{batch\PYGZus{}number} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ceil}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{examples}\PYG{p}{)} \PYG{o}{/} \PYG{n}{mini\PYGZus{}batch\PYGZus{}size}\PYG{p}{)))]}
            \PYG{c+c1}{\PYGZsh{} backpropogate for each mini batch}
            \PYG{k}{for} \PYG{n}{mini\PYGZus{}batch} \PYG{o+ow}{in} \PYG{n}{mini\PYGZus{}batches}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{backpropogate}\PYG{p}{(}\PYG{n}{mini\PYGZus{}batch}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{p}{)}
\end{Verbatim}
